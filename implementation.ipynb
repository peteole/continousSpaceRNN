{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 11:22:05.659911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-30 11:22:05.788667: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-30 11:22:06.443726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-30 11:22:06.443784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-30 11:22:06.443789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/olep/.local/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "/home/olep/.local/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "import tensorflow_graphics as tfg\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "from image_interpolation import ImageInterpolator, ImageSectionRNNCell, generate_2d_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 11:22:11.191709: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-30 11:22:11.191793: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ideapad5pro): /proc/driver/nvidia/version does not exist\n",
      "2022-09-30 11:22:11.193544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16, 20, 2) dtype=float32 (created by layer 'tf.stack_2')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts=tf.keras.Input(shape=(2,))\n",
    "ends=tf.keras.Input(shape=(2,))\n",
    "generate_2d_grid(starts, ends, (16, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test differentiable batched interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "images= tf.keras.Input(shape=(28, 28, 3), name='images')\n",
    "queries = tf.keras.Input(shape=(16, 16, 2), name='queries')\n",
    "interpolated = tfp.math.batch_interp_regular_nd_grid(\n",
    "            queries, [0.0, 0.0], [1.0, 1.0], images, axis=-3)\n",
    "print(interpolated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower range dim 0:  KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name=None), name='tf.linspace_14/linspace/Slice:0', description=\"created by layer 'tf.linspace_14'\")\n",
      "lower range second dim:  KerasTensor(type_spec=TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), name='tf.einsum_2/einsum/Einsum:0', description=\"created by layer 'tf.einsum_2'\")\n",
      "stacked:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 2), dtype=tf.float32, name=None), name='tf.stack_11/stack:0', description=\"created by layer 'tf.stack_11'\")\n",
      "upper stacked:  KerasTensor(type_spec=TensorSpec(shape=(None, 5, 2), dtype=tf.float32, name=None), name='tf.stack_12/stack:0', description=\"created by layer 'tf.stack_12'\")\n",
      "ranges: [<KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_12')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_13')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_14')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_15')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_16')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_17')>, <KerasTensor: shape=(None, 5, 2) dtype=float32 (created by layer 'tf.__operators__.add_18')>]\n",
      "stacked: KerasTensor(type_spec=TensorSpec(shape=(None, 5, 7, 2), dtype=tf.float32, name=None), name='tf.stack_13/stack:0', description=\"created by layer 'tf.stack_13'\")\n"
     ]
    }
   ],
   "source": [
    "starts=tf.keras.Input(shape=(2,))\n",
    "ends=tf.keras.Input(shape=(2,))\n",
    "# starts=tf.Variable([[0.0, 2.0],[0.0, 3.0]])\n",
    "# ends=tf.Variable([[1.0, 4.0],[1.0, 5.0]])\n",
    "nums=[5,7]\n",
    "dim0_range=tf.linspace(starts[:,0],ends[:,0],nums[0],axis=1)\n",
    "\n",
    "print(\"lower range dim 0: \",dim0_range)\n",
    "second_spacial_dim_start=tf.einsum('i,j->ij',starts[:,1],tf.ones((nums[0],)))\n",
    "print(\"lower range second dim: \",second_spacial_dim_start)\n",
    "lower_stacked=tf.stack([dim0_range,second_spacial_dim_start],axis=2)\n",
    "print(\"stacked: \",lower_stacked)\n",
    "second_spacial_dim_end=tf.einsum('i,j->ij',ends[:,1],tf.ones((nums[0],)))\n",
    "upper_stacked=tf.stack([dim0_range,second_spacial_dim_end],axis=2)\n",
    "print(\"upper stacked: \",upper_stacked)\n",
    "\n",
    "ranges=[(1.0-alpha)*lower_stacked+alpha*upper_stacked for alpha in [i/(nums[1]-1) for i in range(nums[1])]]\n",
    "print(\"ranges:\",ranges)\n",
    "meshed_stacked=tf.stack(ranges,axis=2)\n",
    "print(\"stacked:\",meshed_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name=None), name='tf.linspace_10/linspace/Slice:0', description=\"created by layer 'tf.linspace_10'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\")\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.getitem_22), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[0., 1.],\n",
      "       [1., 2.]], dtype=float32)>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.getitem_23), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[0., 1.],\n",
      "       [1., 2.]], dtype=float32)>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "output: tf.Tensor([ 5. 15.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5. 5.]\n",
      " [5. 5.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inp=tf.keras.Input(shape=(2,))\n",
    "interval=tf.linspace(inp[:,0],inp[:,1],10)\n",
    "print(interval)\n",
    "s=tf.reduce_sum(interval,axis=0)\n",
    "print(s)\n",
    "model=tf.keras.Model(inputs=inp,outputs=s)\n",
    "#model.summary()\n",
    "values=tf.Variable([[0.0,1.0],[1.0,2.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(values)\n",
    "    out=model(values)\n",
    "    print('output:',out)\n",
    "    print(tape.gradient(out,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.  1. ]\n",
      "  [1.  1.5]\n",
      "  [1.  2. ]]\n",
      "\n",
      " [[1.5 1. ]\n",
      "  [1.5 1.5]\n",
      "  [1.5 2. ]]\n",
      "\n",
      " [[2.  1. ]\n",
      "  [2.  1.5]\n",
      "  [2.  2. ]]], shape=(3, 3, 2), dtype=float32)\n",
      "(<tf.Tensor 'image_interpolator_1/add:0' shape=(None, 2) dtype=float32>,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"image_interpolator_1\" (type ImageInterpolator).\n\nin user code:\n\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 33, in call  *\n        queries = generate(starts, stops, self.grid_dim)\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 93, in generate  *\n        grid._grid(starts, stops, nums)\n    File \"/home/olep/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py\", line 46, in _grid  *\n        params = [tf.unstack(tensor) for tensor in [starts, stops, nums]]\n\n    ValueError: Cannot infer argument `num` from shape (None, 2)\n\n\nCall arguments received by layer \"image_interpolator_1\" (type ImageInterpolator):\n  • image=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • section=tf.Tensor(shape=(None, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m interpolatorSectionInput\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tfg\u001b[39m.\u001b[39mgeometry\u001b[39m.\u001b[39mrepresentation\u001b[39m.\u001b[39mgrid\u001b[39m.\u001b[39m_grid(starts\u001b[39m=\u001b[39m[\u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m], stops\u001b[39m=\u001b[39m[\u001b[39m2.0\u001b[39m, \u001b[39m2.0\u001b[39m], nums\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m interpolatorOutput\u001b[39m=\u001b[39mImageInterpolator(grid_dim\u001b[39m=\u001b[39;49m(\u001b[39m16\u001b[39;49m, \u001b[39m16\u001b[39;49m))(interpolatorImageInput,interpolatorSectionInput)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezzqpsldm.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, image, section)\u001b[0m\n\u001b[1;32m     11\u001b[0m stops \u001b[39m=\u001b[39m (ag__\u001b[39m.\u001b[39mld(section)[:, :\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(section)[:, \u001b[39m2\u001b[39m:],)\n\u001b[1;32m     12\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(stops))\n\u001b[0;32m---> 13\u001b[0m queries \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(generate), (ag__\u001b[39m.\u001b[39;49mld(starts), ag__\u001b[39m.\u001b[39;49mld(stops), ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mgrid_dim), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mqueries: \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(queries))\n\u001b[1;32m     15\u001b[0m interpolated \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tfp)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mbatch_interp_regular_nd_grid, (ag__\u001b[39m.\u001b[39mld(queries), [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m], [\u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m], ag__\u001b[39m.\u001b[39mld(image)), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_3_193p_.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__generate\u001b[0;34m(starts, stops, nums, name)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstack, ([ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(grid)\u001b[39m.\u001b[39m_grid, (ag__\u001b[39m.\u001b[39mld(starts), ag__\u001b[39m.\u001b[39mld(stops), ag__\u001b[39m.\u001b[39mld(nums)), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m (starts, stops) \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(starts),), \u001b[39mNone\u001b[39;00m, fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(stops),), \u001b[39mNone\u001b[39;00m, fscope)), \u001b[39mNone\u001b[39;00m, fscope)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_3_193p_.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstack, ([ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(grid)\u001b[39m.\u001b[39;49m_grid, (ag__\u001b[39m.\u001b[39;49mld(starts), ag__\u001b[39m.\u001b[39;49mld(stops), ag__\u001b[39m.\u001b[39;49mld(nums)), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39mfor\u001b[39;00m (starts, stops) \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(starts),), \u001b[39mNone\u001b[39;00m, fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(stops),), \u001b[39mNone\u001b[39;00m, fscope)), \u001b[39mNone\u001b[39;00m, fscope)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed4y2b2ay.py:29\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___grid\u001b[0;34m(starts, stops, nums)\u001b[0m\n\u001b[1;32m     27\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     28\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 29\u001b[0m params \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(tensor),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m [ag__\u001b[39m.\u001b[39mld(starts), ag__\u001b[39m.\u001b[39mld(stops), ag__\u001b[39m.\u001b[39mld(nums)]]\n\u001b[1;32m     30\u001b[0m layout \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mlinspace, \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(param)), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(params)), \u001b[39mNone\u001b[39;00m, fscope)]\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed4y2b2ay.py:29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     28\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 29\u001b[0m params \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49munstack, (ag__\u001b[39m.\u001b[39;49mld(tensor),), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m [ag__\u001b[39m.\u001b[39mld(starts), ag__\u001b[39m.\u001b[39mld(stops), ag__\u001b[39m.\u001b[39mld(nums)]]\n\u001b[1;32m     30\u001b[0m layout \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mlinspace, \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(param)), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(params)), \u001b[39mNone\u001b[39;00m, fscope)]\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"image_interpolator_1\" (type ImageInterpolator).\n\nin user code:\n\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 33, in call  *\n        queries = generate(starts, stops, self.grid_dim)\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 93, in generate  *\n        grid._grid(starts, stops, nums)\n    File \"/home/olep/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py\", line 46, in _grid  *\n        params = [tf.unstack(tensor) for tensor in [starts, stops, nums]]\n\n    ValueError: Cannot infer argument `num` from shape (None, 2)\n\n\nCall arguments received by layer \"image_interpolator_1\" (type ImageInterpolator):\n  • image=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • section=tf.Tensor(shape=(None, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "interpolatorImageInput=tf.keras.Input(shape=(28, 28, 1))\n",
    "interpolatorSectionInput=tf.keras.Input(shape=(3,))\n",
    "print(tfg.geometry.representation.grid._grid(starts=[1.0, 1.0], stops=[2.0, 2.0], nums=[3, 3]))\n",
    "interpolatorOutput=ImageInterpolator(grid_dim=(16, 16))(interpolatorImageInput,interpolatorSectionInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `axis` = 0 not in range [0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gridInput\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m grid\u001b[39m=\u001b[39mtfg\u001b[39m.\u001b[39;49mgeometry\u001b[39m.\u001b[39;49mrepresentation\u001b[39m.\u001b[39;49mgrid\u001b[39m.\u001b[39;49mgenerate(starts\u001b[39m=\u001b[39;49m[\u001b[39m1.0\u001b[39;49m, \u001b[39m1.0\u001b[39;49m], stops\u001b[39m=\u001b[39;49m[\u001b[39m2.0\u001b[39;49m, \u001b[39m2.0\u001b[39;49m], nums\u001b[39m=\u001b[39;49m[\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(grid)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py:125\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(starts, stops, nums, name)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[39mreturn\u001b[39;00m _grid(starts, stops, nums)\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mstack([\n\u001b[1;32m    126\u001b[0m       _grid(starts, stops, nums)\n\u001b[1;32m    127\u001b[0m       \u001b[39mfor\u001b[39;00m starts, stops \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tf\u001b[39m.\u001b[39munstack(starts), tf\u001b[39m.\u001b[39munstack(stops))\n\u001b[1;32m    128\u001b[0m   ])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py:126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[39mreturn\u001b[39;00m _grid(starts, stops, nums)\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mstack([\n\u001b[0;32m--> 126\u001b[0m       _grid(starts, stops, nums)\n\u001b[1;32m    127\u001b[0m       \u001b[39mfor\u001b[39;00m starts, stops \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tf\u001b[39m.\u001b[39munstack(starts), tf\u001b[39m.\u001b[39munstack(stops))\n\u001b[1;32m    128\u001b[0m   ])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py:46\u001b[0m, in \u001b[0;36m_grid\u001b[0;34m(starts, stops, nums)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_grid\u001b[39m(starts, stops, nums):\n\u001b[1;32m     28\u001b[0m   \u001b[39m\"\"\"Generates a M-D uniform axis-aligned grid.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[39m  Warning:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m      grid.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m   params \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39munstack(tensor) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m [starts, stops, nums]]\n\u001b[1;32m     47\u001b[0m   layout \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlinspace(\u001b[39m*\u001b[39mparam) \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mparams)]\n\u001b[1;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mstack(tf\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mlayout, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py:46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_grid\u001b[39m(starts, stops, nums):\n\u001b[1;32m     28\u001b[0m   \u001b[39m\"\"\"Generates a M-D uniform axis-aligned grid.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[39m  Warning:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m      grid.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m   params \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49munstack(tensor) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m [starts, stops, nums]]\n\u001b[1;32m     47\u001b[0m   layout \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlinspace(\u001b[39m*\u001b[39mparam) \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mparams)]\n\u001b[1;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mstack(tf\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mlayout, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1723\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(value, num, axis, name)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[39mif\u001b[39;00m value_shape\u001b[39m.\u001b[39mndims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1722\u001b[0m   \u001b[39mif\u001b[39;00m axis \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mvalue_shape\u001b[39m.\u001b[39mndims \u001b[39mor\u001b[39;00m axis \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m value_shape\u001b[39m.\u001b[39mndims:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument `axis` = \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m not in range \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1724\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m-\u001b[39mvalue_shape\u001b[39m.\u001b[39mndims\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalue_shape\u001b[39m.\u001b[39mndims\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1725\u001b[0m   num \u001b[39m=\u001b[39m value_shape\u001b[39m.\u001b[39mdims[axis]\u001b[39m.\u001b[39mvalue\n\u001b[1;32m   1726\u001b[0m \u001b[39mif\u001b[39;00m num \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `axis` = 0 not in range [0, 0)"
     ]
    }
   ],
   "source": [
    "gridInput=tf.keras.Input(shape=(3,))\n",
    "grid=tfg.geometry.representation.grid.generate(starts=[1.0, 1.0], stops=[2.0, 2.0], nums=[3, 3])\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 14:16:07.211653: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-27 14:16:07.211762: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ideapad5pro): /proc/driver/nvidia/version does not exist\n",
      "2022-09-27 14:16:07.213656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 28, 28, 1)\n",
      "(<tf.Tensor 'module_wrapper/rnn/image_section_rnn_cell/image_interpolator_1/add:0' shape=(None, 2) dtype=float32>,)\n",
      "queries:  Tensor(\"module_wrapper/rnn/image_section_rnn_cell/image_interpolator_1/stack:0\", shape=(None, 1), dtype=float32)\n",
      "(<tf.Tensor 'rnn/image_section_rnn_cell/image_interpolator_1/add:0' shape=(None, 2) dtype=float32>,)\n",
      "queries:  Tensor(\"rnn/image_section_rnn_cell/image_interpolator_1/stack:0\", shape=(None, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"image_section_rnn_cell\" \"                 f\"(type ImageSectionRNNCell).\n\nin user code:\n\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 73, in call  *\n        processed_section_values = self.section_processor(section_values)\n    File \"/home/olep/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/olep/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, None)\n    \n    Call arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n      • training=None\n      • mask=None\n\n\nCall arguments received by layer \"image_section_rnn_cell\" \"                 f\"(type ImageSectionRNNCell):\n  • image=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • state_t=(ListWrapper(['tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 32), dtype=float32)']), 'tf.Tensor(shape=(None, 3), dtype=float32)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(repeated_input\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m classifier\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mRNN(ImageSectionRNNCell(grid_dim\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m), units\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m output\u001b[39m=\u001b[39mclassifier(repeated_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m, outputs\u001b[39m=\u001b[39moutput)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer.py:1044\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1042\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1044\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1047\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/engine/sequential.py:393\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m argspec:\n\u001b[1;32m    391\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m training\n\u001b[0;32m--> 393\u001b[0m outputs \u001b[39m=\u001b[39m layer(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(nest\u001b[39m.\u001b[39mflatten(outputs)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    396\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer.py:1044\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1042\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1044\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1047\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/engine/functional.py:1446\u001b[0m, in \u001b[0;36mModuleWrapper.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_mask_arg:\n\u001b[1;32m   1445\u001b[0m   kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1446\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_module, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_method_name)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/rnn/base_rnn.py:553\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[1;32m    549\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    555\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6xdc7qmv.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, image, state_t)\u001b[0m\n\u001b[1;32m     10\u001b[0m (last_lstm_states, section_commands) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(state_t)\n\u001b[1;32m     11\u001b[0m section_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39minterpolator, (ag__\u001b[39m.\u001b[39mld(image), ag__\u001b[39m.\u001b[39mld(section_commands)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m processed_section_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49msection_processor, (ag__\u001b[39m.\u001b[39;49mld(section_values),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     13\u001b[0m (lstm_out, lstm_states) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlstm_cell, (ag__\u001b[39m.\u001b[39mld(processed_section_values), ag__\u001b[39m.\u001b[39mld(last_lstm_states)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m next_section_command \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnext_section_command_computer, (ag__\u001b[39m.\u001b[39mld(lstm_out),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"image_section_rnn_cell\" \"                 f\"(type ImageSectionRNNCell).\n\nin user code:\n\n    File \"/home/olep/Documents/software/continousSpaceRNN/image_interpolation.py\", line 73, in call  *\n        processed_section_values = self.section_processor(section_values)\n    File \"/home/olep/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/olep/.local/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, None)\n    \n    Call arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n      • training=None\n      • mask=None\n\n\nCall arguments received by layer \"image_section_rnn_cell\" \"                 f\"(type ImageSectionRNNCell):\n  • image=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n  • state_t=(ListWrapper(['tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 32), dtype=float32)']), 'tf.Tensor(shape=(None, 3), dtype=float32)')"
     ]
    }
   ],
   "source": [
    "input=tf.keras.Input(shape=(28, 28, 1))\n",
    "num_iterations=20\n",
    "repeated_input = tf.repeat(tf.expand_dims(input,1), repeats=num_iterations, axis=1)\n",
    "print(repeated_input.shape)\n",
    "classifier=keras.Sequential([\n",
    "    tf.keras.layers.RNN(ImageSectionRNNCell(grid_dim=(8, 8), units=32)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "output=classifier(repeated_input)\n",
    "model=tf.keras.Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with the grid interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[4.0666666]]], shape=(1, 1, 1), dtype=float32)\n",
      "tf.Tensor([[[2.0000002 0.6666665]]], shape=(1, 1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "queries = tf.Variable([[[1.1, 1.3]]])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tfp.math.batch_interp_regular_nd_grid(\n",
    "        queries, [0.0, 0.0], [3.0, 3.0], image, axis=-3)\n",
    "    print(z)\n",
    "print(tape.gradient(z, queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>,)\n",
      "queries:  tf.Tensor(\n",
      "[[0. ]\n",
      " [0. ]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [1. ]\n",
      " [1. ]], shape=(6, 1), dtype=float32)\n",
      "Interpolation:  tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [5.]\n",
      "  [5.]\n",
      "  [9.]\n",
      "  [9.]]], shape=(1, 6, 1), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>,)\n",
      "queries:  tf.Tensor(\n",
      "[[0. ]\n",
      " [0. ]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [1. ]\n",
      " [1. ]], shape=(6, 1), dtype=float32)\n",
      "Interpolation:  tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [5.]\n",
      "  [5.]\n",
      "  [9.]\n",
      "  [9.]]], shape=(1, 6, 1), dtype=float32)\n",
      "Sum:  tf.Tensor([30.], shape=(1,), dtype=float32)\n",
      "Gradient of sum w.r.t. section: tf.Tensor([[24. 24. 24.]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 14:27:30.296973: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-09-27 14:27:30.297022: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ideapad5pro): /proc/driver/nvidia/version does not exist\n",
      "2022-09-27 14:27:30.299422: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "image = tf.constant(\n",
    "    [[[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]], [[7.0], [8.0], [9.0]]]])\n",
    "interpolator = ImageInterpolator(grid_dim=(3, 3))\n",
    "print('Interpolation: ', interpolator(image, tf.constant([[0.0, 0.0, 1.0]])))\n",
    "section = tf.Variable([[0.0, 0.0, 1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    interpolation = interpolator(image, section)\n",
    "    print('Interpolation: ', interpolation)\n",
    "    # Take sum alon the last two axes\n",
    "    s = tf.reduce_sum(interpolation, axis=1)\n",
    "    s2 = tf.reduce_sum(s, axis=1)\n",
    "    print(\"Sum: \", s2)\n",
    "print(\"Gradient of sum w.r.t. section:\", tape.gradient(s2, section))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf.unstack_1\" (type TFOpLambda).\n\nCannot infer argument `num` from shape (None, 2)\n\nCall arguments received by layer \"tf.unstack_1\" (type TFOpLambda):\n  • value=tf.Tensor(shape=(None, 2), dtype=float32)\n  • num=None\n  • axis=0\n  • name=unstack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m startsInput\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m stopsInput\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m,))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/olep/Documents/software/continousSpaceRNN/implementation.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tfg\u001b[39m.\u001b[39;49mgeometry\u001b[39m.\u001b[39;49mrepresentation\u001b[39m.\u001b[39;49mgrid\u001b[39m.\u001b[39;49mgenerate(starts\u001b[39m=\u001b[39;49mstartsInput, stops\u001b[39m=\u001b[39;49mstopsInput, nums\u001b[39m=\u001b[39;49m[\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_graphics/geometry/representation/grid.py:127\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(starts, stops, nums, name)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[39mreturn\u001b[39;00m _grid(starts, stops, nums)\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mstack([\n\u001b[1;32m    126\u001b[0m       _grid(starts, stops, nums)\n\u001b[0;32m--> 127\u001b[0m       \u001b[39mfor\u001b[39;00m starts, stops \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tf\u001b[39m.\u001b[39;49munstack(starts), tf\u001b[39m.\u001b[39munstack(stops))\n\u001b[1;32m    128\u001b[0m   ])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/core/tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[39misinstance\u001b[39m(x, keras_tensor\u001b[39m.\u001b[39mKerasTensor)\n\u001b[1;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten([args, kwargs])\n\u001b[1;32m    118\u001b[0m ):\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m TFOpLambda(op)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.unstack_1\" (type TFOpLambda).\n\nCannot infer argument `num` from shape (None, 2)\n\nCall arguments received by layer \"tf.unstack_1\" (type TFOpLambda):\n  • value=tf.Tensor(shape=(None, 2), dtype=float32)\n  • num=None\n  • axis=0\n  • name=unstack"
     ]
    }
   ],
   "source": [
    "startsInput=tf.keras.Input(shape=(2,))\n",
    "\n",
    "stopsInput=tf.keras.Input(shape=(2,))\n",
    "tfg.geometry.representation.grid.generate(starts=startsInput, stops=stopsInput, nums=[3, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
